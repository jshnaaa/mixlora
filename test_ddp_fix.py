#!/usr/bin/env python3
"""
Test script to verify DDP configuration fixes.
"""

import sys
import os

def test_device_map_fix():
    """Test that device_map is disabled for DDP compatibility."""
    print("üîç Testing device_map fix...")

    try:
        # Read train_moe.py and check device_map configuration
        with open('/Users/yzl/ownCode/MixLoRA/custom_training/train_moe.py', 'r') as f:
            content = f.read()

        # Check that device_map is set to None
        if 'device_map=None' in content and '# Let DDP handle device placement' in content:
            print("‚úì device_map set to None for DDP compatibility")
        else:
            print("‚úó device_map not properly configured")
            return False

        # Check that device_map="auto" is not used in actual code (ignore comments)
        import re
        # Find all device_map assignments (not in comments)
        device_map_assignments = re.findall(r'^\s*device_map\s*=\s*[^#\n]*', content, re.MULTILINE)
        auto_assignments = [line for line in device_map_assignments if '"auto"' in line]

        if not auto_assignments:
            print("‚úì No device_map='auto' assignments in code")
        else:
            print(f"‚úó Found device_map='auto' assignments: {auto_assignments}")
            return False

        return True

    except Exception as e:
        print(f"‚úó Error testing device_map fix: {e}")
        return False

def test_ddp_configuration():
    """Test DDP-related configuration."""
    print("\nüîç Testing DDP configuration...")

    try:
        # Read train_moe.py and check DDP settings
        with open('/Users/yzl/ownCode/MixLoRA/custom_training/train_moe.py', 'r') as f:
            content = f.read()

        # Check DDP parameters
        ddp_configs = [
            'ddp_find_unused_parameters=False',
            'ddp_backend="nccl"',
            'ddp_broadcast_buffers=False'
        ]

        for config in ddp_configs:
            if config in content:
                print(f"‚úì Found {config}")
            else:
                print(f"‚úó Missing {config}")
                return False

        return True

    except Exception as e:
        print(f"‚úó Error testing DDP configuration: {e}")
        return False

def test_device_placement():
    """Test device placement logic."""
    print("\nüîç Testing device placement logic...")

    try:
        # Read train_moe.py and check device placement
        with open('/Users/yzl/ownCode/MixLoRA/custom_training/train_moe.py', 'r') as f:
            content = f.read()

        # Check for LOCAL_RANK handling
        if 'local_rank = int(os.environ.get("LOCAL_RANK", 0))' in content:
            print("‚úì LOCAL_RANK environment variable handling")
        else:
            print("‚úó Missing LOCAL_RANK handling")
            return False

        # Check for device assignment
        if 'device = torch.device(f"cuda:{local_rank}")' in content:
            print("‚úì Device assignment based on local_rank")
        else:
            print("‚úó Missing device assignment")
            return False

        # Check for model.to(device)
        if 'self.model = self.model.to(device)' in content:
            print("‚úì Model moved to correct device")
        else:
            print("‚úó Missing model device placement")
            return False

        return True

    except Exception as e:
        print(f"‚úó Error testing device placement: {e}")
        return False

def test_torchrun_command():
    """Test torchrun command configuration."""
    print("\nüîç Testing torchrun command...")

    try:
        # Read run_moe.sh and check torchrun configuration
        with open('/Users/yzl/ownCode/MixLoRA/run_moe.sh', 'r') as f:
            content = f.read()

        # Check torchrun command
        if 'torchrun --nproc_per_node=2 --master_port=29501' in content:
            print("‚úì torchrun command properly configured")
        else:
            print("‚úó torchrun command missing or incorrect")
            return False

        # Check that it's used for multi-GPU training
        if 'echo "Running multi-GPU MoE training with torchrun (DDP)..."' in content:
            print("‚úì torchrun used for multi-GPU training")
        else:
            print("‚úó torchrun not properly integrated")
            return False

        return True

    except Exception as e:
        print(f"‚úó Error testing torchrun command: {e}")
        return False

def test_distributed_environment_simulation():
    """Test distributed environment simulation."""
    print("\nüîç Testing distributed environment simulation...")

    try:
        def simulate_ddp_setup(use_device_map, local_rank):
            """Simulate DDP setup with different configurations."""

            # Simulate the old problematic configuration
            if use_device_map:
                return "Error: device_map conflicts with DDP"

            # Simulate the new fixed configuration
            device = f"cuda:{local_rank}"
            return f"Success: Model on {device}, DDP ready"

        # Test old configuration (should fail)
        old_result = simulate_ddp_setup(use_device_map=True, local_rank=0)
        if "Error" in old_result:
            print("‚úì Old configuration correctly identified as problematic")
        else:
            print("‚úó Old configuration should be problematic")
            return False

        # Test new configuration (should succeed)
        new_result = simulate_ddp_setup(use_device_map=False, local_rank=0)
        if "Success" in new_result:
            print("‚úì New configuration should work")
        else:
            print("‚úó New configuration should succeed")
            return False

        # Test multiple ranks
        for rank in [0, 1]:
            rank_result = simulate_ddp_setup(use_device_map=False, local_rank=rank)
            if f"cuda:{rank}" in rank_result:
                print(f"‚úì Rank {rank} correctly assigned to cuda:{rank}")
            else:
                print(f"‚úó Rank {rank} device assignment incorrect")
                return False

        return True

    except Exception as e:
        print(f"‚úó Error testing distributed simulation: {e}")
        return False

def main():
    """Run all tests."""
    print("üöÄ Testing DDP and distributed training fixes...\n")

    tests = [
        ("Device Map Fix", test_device_map_fix),
        ("DDP Configuration", test_ddp_configuration),
        ("Device Placement Logic", test_device_placement),
        ("Torchrun Command", test_torchrun_command),
        ("Distributed Environment Simulation", test_distributed_environment_simulation),
    ]

    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚úó {test_name} failed with exception: {e}")
            results.append((test_name, False))

    print("\n" + "="*60)
    print("üìã DDP FIX TEST SUMMARY")
    print("="*60)

    passed = 0
    for test_name, result in results:
        status = "‚úì PASS" if result else "‚úó FAIL"
        print(f"{status:<8} {test_name}")
        if result:
            passed += 1

    print(f"\nüéØ Results: {passed}/{len(tests)} tests passed")

    if passed == len(tests):
        print("\nüéâ DDP fixes verified successfully!")
        print("\nüîß Summary of fixes applied:")
        print("1. ‚úÖ Removed device_map='auto' (conflicts with DDP)")
        print("2. ‚úÖ Added proper DDP configuration parameters")
        print("3. ‚úÖ Implemented LOCAL_RANK-based device placement")
        print("4. ‚úÖ Configured NCCL backend for multi-GPU communication")
        print("5. ‚úÖ Disabled buffer broadcasting for memory optimization")
        print("\nüöÄ Distributed MoE training should now work!")
        print("\nüí° Key changes:")
        print("   - device_map=None (let DDP handle placement)")
        print("   - ddp_backend='nccl' (optimized for multi-GPU)")
        print("   - Manual device placement using LOCAL_RANK")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {len(tests) - passed} tests failed.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)