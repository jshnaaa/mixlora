# ğŸ”§ Run Custom Training æ¢¯åº¦é”™è¯¯ä¿®å¤æ€»ç»“

## ğŸ“‹ é—®é¢˜æè¿°

```
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
```

è¿™ä¸ªé”™è¯¯è¡¨ç¤ºåœ¨lossè®¡ç®—æ—¶ï¼ŒæŸä¸ªå¼ é‡ä¸éœ€è¦æ¢¯åº¦ä¸”æ²¡æœ‰æ¢¯åº¦å‡½æ•°ï¼Œå¯¼è‡´åå‘ä¼ æ’­æ— æ³•è¿›è¡Œã€‚

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

åœ¨ `train_mixlora_only=True` æ¨¡å¼ä¸‹ï¼Œå‚æ•°å†»ç»“é€»è¾‘å­˜åœ¨é”™è¯¯ï¼š

### âŒ åŸå§‹é”™è¯¯ä»£ç ï¼š
```python
if any(component in name for component in ['moe_gate', 'experts']) and any(lora_part in name for lora_part in ['lora_A', 'lora_B', 'moe_gate']):
```

**é—®é¢˜ï¼š** `'moe_gate'` åŒæ—¶å‡ºç°åœ¨ä¸¤ä¸ªåˆ—è¡¨ä¸­ï¼Œå¯¼è‡´é€»è¾‘æ··ä¹±ã€‚

## âœ… åº”ç”¨çš„ä¿®å¤

### 1. **ä¿®å¤å‚æ•°å†»ç»“é€»è¾‘**
```python
# ä¿®å¤å‰
if any(component in name for component in ['moe_gate', 'experts']) and any(lora_part in name for lora_part in ['lora_A', 'lora_B', 'moe_gate']):

# ä¿®å¤å
is_moe_component = (
    ('moe_gate' in name) or  # Router
    ('experts' in name and any(lora_part in name for lora_part in ['lora_A', 'lora_B']))  # Expert LoRA
)

if is_moe_component:
```

### 2. **æ·»åŠ é›¶å¯è®­ç»ƒå‚æ•°æ£€æŸ¥**
```python
# Verify that we have trainable parameters
if trainable_params_count == 0:
    raise ValueError("âŒ No trainable parameters found! All parameters are frozen. Check parameter freezing logic.")

self.logger.info(f"âœ… Verified {trainable_params_count:,} trainable parameters")
```

### 3. **æ·»åŠ DDPä¸€è‡´æ€§éªŒè¯**
```python
# Additional validation for DDP training
if hasattr(self.model, 'module'):
    # In DDP, check the underlying module as well
    ddp_trainable_count = sum(p.numel() for p in self.model.module.parameters() if p.requires_grad)
    if ddp_trainable_count != trainable_params_count:
        self.logger.warning(f"âš ï¸  DDP trainable parameter count mismatch: {ddp_trainable_count} vs {trainable_params_count}")
    else:
        self.logger.info(f"âœ… DDP parameter consistency verified")
```

### 4. **æ·»åŠ è®­ç»ƒå‰æ¢¯åº¦çŠ¶æ€éªŒè¯**
```python
def _verify_gradient_state(self):
    """Verify that the model is in a valid state for training."""
    self.logger.info("ğŸ” Verifying gradient state before training...")

    # Check that we have trainable parameters
    trainable_params = [p for p in self.model.parameters() if p.requires_grad]
    if len(trainable_params) == 0:
        raise ValueError("âŒ No trainable parameters found before training!")

    # Test forward pass with a dummy input to ensure gradients can flow
    # ... (è¯¦ç»†å®ç°è§ä»£ç )
```

## ğŸ“Š ä¿®å¤æ•ˆæœéªŒè¯

### å‚æ•°å†»ç»“æ¨¡æ‹Ÿæµ‹è¯•ï¼š
```
âœ… TRAINABLE: mixlora.layers.0.mlp.moe_gate.weight                    (Router)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.0.gate_proj.lora_A.weight  (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.0.gate_proj.lora_B.weight  (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.0.up_proj.lora_A.weight    (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.0.up_proj.lora_B.weight    (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.0.down_proj.lora_A.weight  (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.0.down_proj.lora_B.weight  (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.1.gate_proj.lora_A.weight  (Expert LoRA)
âœ… TRAINABLE: mixlora.layers.0.mlp.experts.1.gate_proj.lora_B.weight  (Expert LoRA)

â„ï¸  FROZEN: model.layers.0.self_attn.q_proj.weight                    (Base model)
â„ï¸  FROZEN: model.layers.0.mlp.gate_proj.weight                       (Base model)
â„ï¸  FROZEN: lm_head.weight                                            (Base model)
â„ï¸  FROZEN: model.embed_tokens.weight                                 (Base model)
```

### ç»“æœï¼š
- **Trainable parameters**: 9 ä¸ª âœ…
- **Frozen parameters**: 7 ä¸ª âœ…
- **Logic validation**: é€šè¿‡ âœ…

## ğŸš€ ä¿®å¤çš„å…³é”®ç‚¹

1. **æ¸…æ™°åˆ†ç¦»**: Router (`moe_gate`) å’Œ Expert LoRA ç»„ä»¶çš„æ£€æµ‹é€»è¾‘
2. **æ—©æœŸéªŒè¯**: åœ¨è®­ç»ƒå¼€å§‹å‰éªŒè¯å¯è®­ç»ƒå‚æ•°å­˜åœ¨
3. **DDPå…¼å®¹**: ç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„å‚æ•°ä¸€è‡´æ€§
4. **æ¢¯åº¦æµ‹è¯•**: é€šè¿‡dummy forward passéªŒè¯æ¢¯åº¦æµ

## ğŸ’¡ é¢„æœŸç»“æœ

ä¿®å¤åï¼Œ`run_custom_training.sh` åº”è¯¥èƒ½å¤Ÿï¼š
- âœ… æ­£ç¡®å†»ç»“åŸºç¡€æ¨¡å‹å‚æ•°
- âœ… åªè®­ç»ƒMixLoRA MoEç»„ä»¶ (router + expert LoRA)
- âœ… é¿å… "element 0 of tensors does not require grad" é”™è¯¯
- âœ… åœ¨å•GPUå’ŒåŒGPUæ¨¡å¼ä¸‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

ä¿®å¤å·²è‡ªåŠ¨åº”ç”¨åˆ° `custom_training/train_mixlora_custom.py`ã€‚

è¿è¡Œè®­ç»ƒï¼š
```bash
./run_custom_training.sh llama 2 2  # LLaMA, CulturalBench, åŒGPU
./run_custom_training.sh qwen 3 1   # Qwen, NormAD, å•GPU
```

è®­ç»ƒå°†é»˜è®¤ä½¿ç”¨ `--train_mixlora_only` æ¨¡å¼ï¼Œåªè®­ç»ƒMoEç»„ä»¶ã€‚