# ğŸ”§ Run MoE è®¾å¤‡ä¸ä¸€è‡´é”™è¯¯ä¿®å¤æ€»ç»“

## ğŸ“‹ é—®é¢˜æè¿°

```
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu!
(when checking argument for argument mat2 in method wrapper_CUDA_mm)
```

**é”™è¯¯ä½ç½®ï¼š** `moe/moe_model.py` ç¬¬151è¡Œï¼Œ`lora_gate.lora_forward(gate_states, hidden_states)`

## ğŸ” æ ¹æœ¬åŸå› åˆ†æ

åœ¨DDPï¼ˆåˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œï¼‰è®­ç»ƒä¸­ï¼š

1. **æ¨¡å‹åŠ è½½é˜¶æ®µ**ï¼šåŸºç¡€æ¨¡å‹è¢«åŠ è½½åˆ°æŒ‡å®šè®¾å¤‡
2. **MoEæ³¨å…¥é˜¶æ®µ**ï¼šExpert LoRAç»„ä»¶åŸºäºå½“æ—¶çš„è®¾å¤‡çŠ¶æ€åˆ›å»º
3. **è®¾å¤‡ç§»åŠ¨é˜¶æ®µ**ï¼š`model.to(device)` ç§»åŠ¨ä¸»æ¨¡å‹ï¼Œä½†MoEç»„ä»¶å¯èƒ½æœªåŒæ­¥
4. **è®­ç»ƒé˜¶æ®µ**ï¼šExpert LoRAç»„ä»¶åœ¨é”™è¯¯è®¾å¤‡ä¸Šï¼Œå¯¼è‡´è®¾å¤‡ä¸åŒ¹é…é”™è¯¯

### å…·ä½“é—®é¢˜ï¼š
- Expert LoRAå±‚ï¼ˆlora_A, lora_Bï¼‰å¯èƒ½åœ¨CPUæˆ–é”™è¯¯çš„GPUä¸Š
- Shared Expertç»„ä»¶å¯èƒ½åœ¨é”™è¯¯è®¾å¤‡ä¸Š
- Router gateæƒé‡å¯èƒ½åœ¨é”™è¯¯è®¾å¤‡ä¸Š
- DDPç¯å¢ƒä¸­ä¸åŒrankçš„è®¾å¤‡åˆ†é…ä¸ä¸€è‡´

## âœ… åº”ç”¨çš„ä¿®å¤æ–¹æ¡ˆ

### 1. **æ·»åŠ MoEå±‚å¼•ç”¨å­˜å‚¨** (`moe/moe_model.py`)
```python
# åœ¨ _inject_moe_mlp_module å‡½æ•°ä¸­
# Replace the original forward function
mlp_layer.forward = moe_layer.forward

# Store reference to MoE layer for device synchronization
mlp_layer._moe_layer = moe_layer
```

**ç›®çš„ï¼š** ä¿å­˜MoEå±‚å¼•ç”¨ï¼Œä¾¿äºåç»­è®¾å¤‡åŒæ­¥è®¿é—®

### 2. **å®ç°è®¾å¤‡åŒæ­¥æ–¹æ³•** (`custom_training/train_moe.py`)
```python
def _sync_moe_components_to_device(self, device):
    """Ensure all MoE components are on the correct device."""
    logger.info(f"ğŸ”„ Syncing MoE components to device: {device}")

    components_moved = 0
    for layer_idx, layer in enumerate(self.model.model.layers):
        if hasattr(layer.mlp, '_moe_layer'):
            moe_layer = layer.mlp._moe_layer

            # Move expert LoRA layers to device
            for expert_key, expert_lora in moe_layer.experts_.items():
                if hasattr(expert_lora, 'lora_A'):
                    expert_lora.lora_A = expert_lora.lora_A.to(device)
                    components_moved += 1
                if hasattr(expert_lora, 'lora_B'):
                    expert_lora.lora_B = expert_lora.lora_B.to(device)
                    components_moved += 1

            # Move shared expert components to device
            if hasattr(moe_layer, 'shared_experts') and moe_layer.shared_experts:
                for shared_key, shared_expert in moe_layer.shared_experts.items():
                    if hasattr(shared_expert, 'lora_A'):
                        shared_expert.lora_A = shared_expert.lora_A.to(device)
                        components_moved += 1
                    if hasattr(shared_expert, 'lora_B'):
                        shared_expert.lora_B = shared_expert.lora_B.to(device)
                        components_moved += 1
                    if hasattr(shared_expert, 'dropout'):
                        shared_expert.dropout = shared_expert.dropout.to(device)
                        components_moved += 1

            # Move router gate to device if it exists
            if hasattr(moe_layer, 'gate_') and moe_layer.gate_ is not None:
                if torch.is_tensor(moe_layer.gate_):
                    moe_layer.gate_ = moe_layer.gate_.to(device)
                    components_moved += 1

    logger.info(f"âœ… Moved {components_moved} MoE components to device: {device}")
```

### 3. **é›†æˆè®¾å¤‡åŒæ­¥åˆ°æ¨¡å‹åˆå§‹åŒ–** (`custom_training/train_moe.py`)
```python
# Ensure model is on correct device for DDP
if torch.cuda.is_available():
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    device = torch.device(f"cuda:{local_rank}")
    self.model = self.model.to(device)
    logger.info(f"Model moved to device: {device}")

    # Ensure all MoE components are also on the correct device
    self._sync_moe_components_to_device(device)
```

**æ—¶æœºï¼š** åœ¨æ¨¡å‹ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡åç«‹å³åŒæ­¥MoEç»„ä»¶

## ğŸ“Š ä¿®å¤æ•ˆæœéªŒè¯

### è®¾å¤‡åŒæ­¥æµ‹è¯•ç»“æœï¼š
```
âœ… Device Sync Implementation - PASS
âœ… MoE Model Reference - PASS
âœ… Device Consistency Checks - PASS
âœ… Device Error Simulation - PASS
âœ… DDP Specific Fixes - PASS
```

### åŒæ­¥çš„ç»„ä»¶ï¼š
- âœ… Expert LoRA A/B layers
- âœ… Shared Expert LoRA A/B layers
- âœ… Shared Expert dropout layers
- âœ… Router gate weights
- âœ… æ‰€æœ‰MoEç›¸å…³ç»„ä»¶

## ğŸ”§ ä¿®å¤æµç¨‹

```
æ¨¡å‹åˆ›å»º â†’ MoEæ³¨å…¥ â†’ model.to(device) â†’ _sync_moe_components_to_device()
    â†“
æ‰€æœ‰ç»„ä»¶åœ¨åŒä¸€è®¾å¤‡ â†’ è®­ç»ƒæ­£å¸¸è¿›è¡Œï¼Œæ— è®¾å¤‡é”™è¯¯
```

## ğŸš€ é¢„æœŸç»“æœ

ä¿®å¤åï¼Œ`run_moe.sh` åº”è¯¥èƒ½å¤Ÿï¼š
- âœ… åœ¨DDPæ¨¡å¼ä¸‹æ­£å¸¸è¿è¡Œï¼ˆåŒGPUï¼‰
- âœ… åœ¨å•GPUæ¨¡å¼ä¸‹æ­£å¸¸è¿è¡Œ
- âœ… é¿å… "Expected all tensors to be on the same device" é”™è¯¯
- âœ… æ­£ç¡®å¤„ç†Expert LoRAå’ŒShared Expertçš„è®¾å¤‡åˆ†é…
- âœ… ç¡®ä¿Router gateåœ¨æ­£ç¡®è®¾å¤‡ä¸Š

## ğŸ’¡ å…³é”®æŠ€æœ¯ç‚¹

1. **å¼•ç”¨å­˜å‚¨**ï¼šé€šè¿‡ `mlp_layer._moe_layer` ä¿å­˜MoEå±‚å¼•ç”¨
2. **é€’å½’åŒæ­¥**ï¼šéå†æ‰€æœ‰å±‚çš„æ‰€æœ‰MoEç»„ä»¶
3. **ç»„ä»¶è¯†åˆ«**ï¼šæ­£ç¡®è¯†åˆ«Expert LoRAã€Shared Expertã€Router gate
4. **DDPå…¼å®¹**ï¼šä¸DDPçš„è®¾å¤‡åˆ†é…ç­–ç•¥å…¼å®¹
5. **æ—¶æœºæ§åˆ¶**ï¼šåœ¨æ¨¡å‹è®¾å¤‡ç§»åŠ¨åç«‹å³æ‰§è¡ŒåŒæ­¥

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

ä¿®å¤å·²è‡ªåŠ¨åº”ç”¨ï¼Œç°åœ¨å¯ä»¥æ­£å¸¸è¿è¡Œï¼š

```bash
# åŒGPU DDPè®­ç»ƒï¼ˆä¹‹å‰ä¼šæŠ¥è®¾å¤‡é”™è¯¯ï¼‰
./run_moe.sh llama 2 2 true

# å•GPUè®­ç»ƒ
./run_moe.sh qwen 3 1 true

# æ— å…±äº«ä¸“å®¶æ¨¡å¼
./run_moe.sh llama 2 2 false
```

è®¾å¤‡åŒæ­¥å°†è‡ªåŠ¨è¿›è¡Œï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ã€‚